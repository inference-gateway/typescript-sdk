# MCP Examples

This directory contains comprehensive examples demonstrating how to use the Inference Gateway SDK with Model Context Protocol (MCP) tools in a multi-provider architecture. Each example showcases different aspects of MCP tool integration.

## üöÄ Quick Start

### Run Specific Examples

```bash

# Run interactive specialized agents
npm run example:nextjs-agent      # ü§ñ Next.js development agent
npm run example:vite-agent        # ‚ö° Vite application agent
npm run example:kubernetes-agent  # ‚ò∏Ô∏è Kubernetes operations agent
```

## Architecture

This example uses Docker Compose to orchestrate:

- **Inference Gateway** - Main API gateway with MCP support enabled
- **MCP Filesystem Server** - Provides file system operations (restricted to `/shared` and `/tmp`)
- **MCP Web Search Server** - Simulated web search and URL fetching
- **Optional Ollama** - Local model inference (when using `--profile with-ollama`)

## Important: Filesystem Access

The MCP filesystem server is configured with restricted access for security:

- **`/shared`** - Read-Write directory for shared files between the host and container
- **`/tmp`** - Read-write directory for temporary files inside of the container

The AI will only be able to access these directories. This prevents unauthorized access to system files.

## Sample Data

The `/shared` directory contains example files for testing.

## ü§ñ Interactive Development Agents

This repository includes three specialized interactive agents that use Context7 MCP tools for up-to-date documentation and best practices:

### ‚ö° Vite Agent (`example-vite-agent.ts`)

**Purpose:** Creates lightning-fast modern Vite applications with optimal configuration.

**Specializes in:**

- React, Vue, Svelte, and vanilla JavaScript/TypeScript projects
- Modern build tooling and optimization
- Fast development server with HMR
- Vitest testing setup
- TypeScript configuration
- CSS preprocessing (Tailwind, Sass, PostCSS)

**Run with:** `npm run example:vite-agent`

**Example requests:**

- "Create a React + TypeScript app with Vite and Tailwind CSS"
- "Build a Vue 3 dashboard with Vite, Vitest, and component library"
- "Make a Svelte SPA with Vite and optimal build configuration"

### ‚ò∏Ô∏è Kubernetes Agent (`example-kubernetes-agent.ts`)

**Purpose:** Handles Kubernetes cluster operations and container orchestration.

**Specializes in:**

- Production-ready YAML manifests
- Deployment strategies (blue-green, canary)
- RBAC and security policies
- Service mesh configuration
- Monitoring and observability
- Scaling and resource management
- CI/CD pipeline integration

**Run with:** `npm run example:kubernetes-agent`

**Example requests:**

- "Deploy a scalable web application with load balancing and auto-scaling"
- "Create a microservices architecture with service mesh and monitoring"
- "Set up RBAC and network policies for multi-tenant cluster"
- "Configure GitOps deployment pipeline with ArgoCD"

### ü§ñ Next.js Agent (`example-nextjs-agent.ts`)

**Purpose:** Creates modern Next.js applications with App Router and latest features.

**Specializes in:**

- Next.js 13+ App Router architecture
- Server Components and streaming
- TypeScript and modern tooling
- Performance optimization
- SEO and accessibility
- Production deployment

**Run with:** `npm run example:nextjs-agent`

**Example requests:**

- "Create a Next.js blog with TypeScript and Tailwind CSS"
- "Build a React dashboard with charts and data visualization"
- "Make a full-stack Next.js app with authentication and database"

### üß∞ Context7 Integration

All agents use Context7 MCP tools to:

- **Fetch Latest Documentation** - Get current best practices and API references
- **Resolve Library Dependencies** - Find compatible package versions
- **Access Code Examples** - Retrieve real-world implementation patterns
- **Stay Current** - Use up-to-date information instead of static training data

Each agent maintains conversation history and provides interactive help. Use `clear` to reset, `help` for guidance, or `exit` to quit.

## Setup Instructions

### Prerequisites

- Docker and Docker Compose installed
- API key for at least one provider (OpenAI, Groq, Anthropic, etc.)

### 1. Environment Configuration

Copy the example environment file:

```bash
cp .env.example .env
```

Configure your environment:

```bash
# Required: Set your provider and model
PROVIDER=openai
LLM=gpt-4o

# Required: Add your API keys (at least one)
OPENAI_API_KEY=your_key_here
GROQ_API_KEY=your_key_here
ANTHROPIC_API_KEY=your_key_here
DEEPSEEK_API_KEY=your_key_here

# Optional: Inference Gateway configuration
EXPOSE_MCP=true
```

### 2. Start Infrastructure

Start the MCP infrastructure:

```bash
npm run compose:up
```

Wait for all services to be healthy:

```bash
docker-compose ps
```

### 3. Install Dependencies

```bash
npm install
```

### 4. Run Examples

Choose any example to run:

```bash
# Start with the basic example
npm run start

# Or run specific examples
npm run run:advanced
npm run run:nextjs
npm run run:tool-demo
```

```bash
cp .env.example .env
```

### 1. Start the MCP Infrastructure

Start all services using Docker Compose:

```bash
npm run compose:up
```

This will start:

- Inference Gateway on port 8080
- MCP Filesystem server on port 3000
- MCP Web Search server on port 3001

### 2. Install Dependencies

```bash
npm install
```

### 3. Configure Provider and Model

Set your preferred provider and model:

```bash
export PROVIDER=groq
export LLM=meta-llama/llama-3.3-70b-versatile
```

Or for OpenAI:

```bash
export PROVIDER=openai
export LLM=gpt-4o
```

### 4. Verify MCP Setup

Test that MCP tools are working correctly:

```bash
npx tsx test-mcp-tools.ts
```

### 5. Run Examples

Run the main example:

```bash
npm start
```

Or run the focused filesystem demo:

```bash
npx tsx filesystem-demo.ts
```

## Available Examples

- `npm run example:nextjs-agent` - Next.js application generator
- `npm run example:vite-agent` - Vite application generator
- `npm run example:kubernetes-agent` - Kubernetes deployment example

## Available Commands

- `npm run compose:up` - Start all services in background
- `npm run compose:down` - Stop all services
- `npm run compose:logs` - View logs from all services

## Key Features Demonstrated

### Enhanced Tool Handling

The examples now showcase:

1. **Separate MCP and Regular Tool Handlers**:

   - `onMCPTool` - Handles tools from MCP servers
   - `onTool` - Handles client-provided tools
   - Enhanced logging with tool IDs and formatted arguments

2. **Comprehensive Stream Callbacks**:

   - `onOpen` - Connection established
   - `onContent` - Streaming content
   - `onMCPTool` - MCP tool calls with detailed logging
   - `onUsageMetrics` - Token usage tracking
   - `onFinish` - Stream completion

3. **Enhanced Error Handling**:

   - Graceful handling of malformed JSON in tool arguments
   - Multiple fallback field names for URL parsing (`url`, `target_url`, `webpage_url`)
   - Detailed logging of parse errors and missing parameters

4. **Tool Argument Debugging**:
   - Raw argument display for troubleshooting
   - Structured argument parsing with error recovery
   - Issue identification for incomplete tool schemas

### Known Issues and Workarounds

The examples demonstrate both working features and current limitations:

**‚úÖ Working:**

- File system operations (read_file, write_file, list_directory, etc.)
- Complete argument passing with path, content, and mcpServer parameters
- Robust error handling and logging

**‚ùå Current Issues:**

- Web tools (fetch_url, search_web) receive incomplete arguments
- Missing URL parameters due to incomplete schema exposure
- LLM receives partial tool definitions from inference gateway

**üîß Demonstrated Solutions:**

- Enhanced argument parsing with multiple field fallbacks
- Graceful error handling when tools fail
- Comprehensive logging to identify root causes
- Tool schema inspection utilities

See `MCP_IMPROVEMENT_SUMMARY.md` for detailed analysis and findings.

- `onError` - Error handling

3. **Better Tool Call Visualization**:
   - Tool call counting
   - Formatted argument display
   - Tool execution tracking
   - Performance metrics

## üÜï Next.js App Creator Example

The `nextjs-example.ts` demonstrates a powerful real-world use case:

### What it does:

1. **Fetches Official Documentation** - Uses MCP web tools to get the latest Next.js docs
2. **Creates Complete App Structure** - Builds a production-ready Next.js application
3. **Follows Best Practices** - Uses the fetched documentation to ensure current patterns
4. **TypeScript Setup** - Includes proper TypeScript configuration
5. **Modern Features** - Implements App Router, Server Components, and latest Next.js features

### Features Demonstrated:

- **Documentation-Driven Development** - AI reads official docs before coding
- **Complex File Operations** - Creates entire application structures
- **Web Content + File Operations** - Combines multiple MCP tool types
- **Production-Ready Output** - Generates runnable Next.js applications

### Run the Example:

```bash
npm run nextjs
```

The AI will:

- Fetch Next.js documentation from https://nextjs.org/docs
- Create a complete application in `/tmp/nextjs-app/`
- Include package.json, configs, pages, components, and README
- Follow the latest Next.js best practices and conventions

## Example Prompts to Try

Once the example is running, you can ask the AI:

1. **File Operations Chain:**

   ```
   "Create a JSON config file at /tmp/config.json with sample data, read it back, and list the directory"
   ```

2. **Multi-step Analysis:**

   ```
   "Read the sales data from /shared/sample_sales_data.csv, analyze trends, and create a summary report"
   ```

3. **Web Research:**

   ```
   "Fetch content from https://httpbin.org/json and tell me what information it contains"
   ```

4. **Documentation-Based Development:**

   ```
   "Create a React component library by first fetching React documentation, then building reusable components"
   ```

5. **Complex File Tasks:**
   ```
   "Create a todo list with 5 tasks, save it to /tmp/todo.txt, then read it back and add 2 more tasks"
   ```

## Example Output

The example will demonstrate:

```
=== MCP Tools Example ===

üîç Checking gateway health...
Gateway health: ‚úÖ Healthy

üìã Listing available MCP tools...
Found 9 MCP tools:

1. read_file
   Description: Read content from a file
   Server: http://mcp-filesystem:3000/mcp
   Schema: {
     "type": "object",
     "properties": {
       "file_path": {
         "type": "string",
         "description": "Path to the file to read"
       }
     },
     "required": ["file_path"]
   }

2. write_file
   Description: Write content to a file
   Server: http://mcp-filesystem:3000/mcp
   Schema: {
     "type": "object",
     "properties": {
       "file_path": {
         "type": "string",
         "description": "Path to the file to write"
       },
       "content": {
         "type": "string",
         "description": "Content to write to the file"
       }
     },
     "required": ["file_path", "content"]
   }

3. fetch_url
   Description: Fetch content from a URL
   Server: http://mcp-web-search:3001/mcp
   Schema: {
     "type": "object",
     "properties": {
       "url": {
         "type": "string",
         "description": "The URL to fetch content from"
       },
       "timeout": {
         "type": "number",
         "description": "Request timeout in milliseconds"
       }
     },
     "required": ["url"]
   }

4. search_web
   Description: Search the web and return results
   Server: http://mcp-web-search:3001/mcp
   Schema: {
     "type": "object",
     "properties": {
       "query": {
         "type": "string",
         "description": "Search query"
       },
       "limit": {
         "type": "number",
         "description": "Number of results to return"
       }
     },
     "required": ["query"]
   }

=== Example 1: File Operations with MCP ===

üöÄ Starting file reading example...
I'll help you read the contents of /tmp/example.txt file.

üîß Tool called: read_file
üìù Arguments: {"file_path": "/tmp/example.txt"}

The file contains:
Hello from MCP filesystem server!
This is a sample file for testing MCP file operations.
Created at: Mon May 27 10:30:00 UTC 2025

‚úÖ File reading example completed
```

## MCP Servers Included

### Filesystem Server

- **Purpose**: File system operations (read, write, list directories)
- **Port**: 3000
- **Tools**: `read_file`, `write_file`, `list_directory`
- **Allowed Directories**: `/shared`, `/tmp`

### Web Search Server

- **Purpose**: Web content fetching and search simulation
- **Port**: 3001
- **Tools**: `fetch_url`, `search_web`
- **Features**: HTTP requests, basic content extraction

### Context7 Server

- **Purpose**: Library documentation and context resolution
- **Port**: 3002
- **Tools**: `resolve_library_id`, `get_library_docs`, `search_libraries`
- **Features**: Library search, documentation retrieval, version management

## Supported Providers

All Inference Gateway providers work with MCP tools:

- `openai` - GPT models with excellent tool calling
- `groq` - Fast inference with Llama and Mixtral models
- `anthropic` - Claude models with strong reasoning
- `ollama` - Local models (use `--profile with-ollama`)
- `cohere` - Command models
- `deepseek` - DeepSeek models
- `cloudflare` - Workers AI models

## Using with Local Models (Ollama)

To include Ollama for local model inference:

```bash
docker-compose --profile with-ollama up -d
```

Then pull a model:

```bash
docker exec -it mcp_ollama_1 ollama pull llama3.2
```

Set environment variables:

```bash
export PROVIDER=ollama
export LLM=llama3.2
```

## Troubleshooting

### MCP Tools Not Available

If you see "No MCP tools available":

1. Check if MCP servers are running:

   ```bash
   docker-compose ps
   ```

2. Verify MCP server logs:

   ```bash
   npm run compose:logs
   ```

3. Ensure the Inference Gateway can reach MCP servers:
   ```bash
   curl -X POST http://localhost:8080/mcp/tools/list -H "Content-Type: application/json" -d '{}'
   ```

### Gateway Health Check Fails

If the gateway appears unhealthy:

1. Check gateway logs:

   ```bash
   docker-compose logs inference-gateway
   ```

2. Verify API keys are set in `.env` file
3. Test direct connection:
   ```bash
   curl http://localhost:8080/health
   ```

### Tool Calls Not Working

If tool calls fail during conversations:

1. Verify the model supports tool calling (GPT-4, Claude, etc.)
2. Check MCP server connectivity through the gateway:

   ```bash
   curl -X POST http://localhost:8080/mcp/tools/list \
     -H "Content-Type: application/json" \
     -d '{}'
   ```

3. Enable debug logging by adding to docker-compose.yml:
   ```yaml
   environment:
     - LOG_LEVEL=debug
   ```

## Custom MCP Servers

To add your own MCP server:

1. Add service to `docker-compose.yml`:

   ```yaml
   my-custom-mcp:
     image: my-mcp-server:latest
     ports:
       - '3002:3002'
     networks:
       - inference-network
   ```

2. Update Inference Gateway environment:

   ```yaml
   MCP_SERVERS: 'http://mcp-filesystem:3000/mcp,http://mcp-web-search:3001/mcp,http://my-custom-mcp:3002/mcp'
   ```

3. Restart services:
   ```bash
   npm run compose:down
   npm run compose:up
   ```

## Notes

- MCP tools are called automatically by AI models when relevant to the conversation
- Tool schemas are defined by the MCP servers and exposed through the `/mcp/tools` endpoint
- Each MCP server can provide multiple tools with different capabilities
- The Inference Gateway acts as a bridge between AI models and MCP tools
- Tool execution is streamed in real-time during conversations
- File operations are sandboxed to allowed directories for security

## Resources

- [Model Context Protocol Specification](https://modelcontextprotocol.io/)
- [Inference Gateway Documentation](https://github.com/inference-gateway/inference-gateway)
- [Official MCP Servers](https://github.com/modelcontextprotocol/servers)

## HTTP Examples

This section provides practical HTTP examples for interacting with MCP tools through the Inference Gateway. These examples are useful for testing, debugging, or integrating with other systems. **Note: Always interact with MCP tools through the Inference Gateway, not directly with MCP servers.**

### Prerequisites for HTTP Examples

Make sure the MCP infrastructure is running with MCP tools exposed:

```bash
npm run compose:up
```

**Important:** Ensure the Inference Gateway is started with `EXPOSE_MCP=true` environment variable to enable MCP endpoints.

### 1. Health Checks

#### Check Inference Gateway Health

```bash
curl -X GET http://localhost:8080/health
```

**Response:**

```json
{
  "status": "healthy",
  "timestamp": "2025-05-28T10:30:00Z",
  "version": "1.0.0"
}
```

### 2. MCP Tool Discovery

#### List All Available MCP Tools via Inference Gateway

```bash
curl -X GET http://localhost:8080/v1/mcp/tools
```

**Response:**

```json
{
  "tools": [
    {
      "name": "read_file",
      "description": "Read content from a file",
      "input_schema": {
        "type": "object",
        "properties": {
          "file_path": {
            "type": "string",
            "description": "Path to the file to read"
          }
        },
        "required": ["file_path"]
      }
    },
    {
      "name": "write_file",
      "description": "Write content to a file",
      "input_schema": {
        "type": "object",
        "properties": {
          "file_path": {
            "type": "string",
            "description": "Path to the file to write"
          },
          "content": {
            "type": "string",
            "description": "Content to write to the file"
          }
        },
        "required": ["file_path", "content"]
      }
    },
    {
      "name": "fetch_url",
      "description": "Fetch content from a URL",
      "input_schema": {
        "type": "object",
        "properties": {
          "url": {
            "type": "string",
            "description": "The URL to fetch content from"
          },
          "timeout": {
            "type": "number",
            "description": "Request timeout in milliseconds"
          }
        },
        "required": ["url"]
      }
    }
  ]
}
```

### 3. Using MCP Tools in Chat Completions

MCP tools are executed automatically by AI models during chat completions. You don't call them directly - instead, you include them in the `tools` array of a chat completion request.

#### Basic Chat Completion with MCP Tools

```bash
curl -X POST http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "groq/llama-3.3-70b-versatile",
    "messages": [
      {
        "role": "system",
        "content": "You are a helpful assistant that can read files using available tools."
      },
      {
        "role": "user",
        "content": "Can you read the contents of /tmp/example.txt?"
      }
    ]
  }'
```

#### Web Scraping with MCP Tools

```bash
curl -X POST http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "openai/gpt-4o",
    "messages": [
      {
        "role": "system",
        "content": "You are a helpful assistant that can fetch web content."
      },
      {
        "role": "user",
        "content": "Can you fetch the content from https://httpbin.org/json?"
      }
    ]
  }'
```

### 4. Streaming Chat Completions with MCP Tools

```bash
curl -X POST http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "anthropic/claude-3-haiku-20240307",
    "messages": [
      {
        "role": "user",
        "content": "Help me analyze the file /shared/data.csv by reading it first"
      }
    ],
    "stream": true
  }'
```

### 5. Multi-Tool Conversations

```bash
curl -X POST http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "groq/mixtral-8x7b-32768",
    "messages": [
      {
        "role": "system",
        "content": "You are a research assistant with access to file operations and web content fetching."
      },
      {
        "role": "user",
        "content": "Research information about AI from https://openai.com and save a summary to /tmp/ai-research.txt"
      }
    ]
  }'
```
