services:
  inference-gateway:
    image: ghcr.io/inference-gateway/inference-gateway:latest
    ports:
      - '8080:8080'
    environment:
      # General settings
      ENVIRONMENT: production

      # Enable MCP support
      MCP_ENABLE: 'true'
      MCP_EXPOSE: 'true'
      MCP_SERVERS: 'http://mcp-filesystem:3000/mcp,http://mcp-context7:3002/mcp,http://mcp-npm:3003/mcp,http://mcp-memory:3004/mcp,http://mcp-brave-search:3005/mcp'

      # Server settings
      SERVER_HOST: '0.0.0.0'
      SERVER_PORT: '8080'

      # Provider settings - Add your API keys here
      GROQ_API_URL: 'https://api.groq.com/openai/v1'
      GROQ_API_KEY: '${GROQ_API_KEY:-}'

      OPENAI_API_URL: 'https://api.openai.com/v1'
      OPENAI_API_KEY: '${OPENAI_API_KEY:-}'

      ANTHROPIC_API_URL: 'https://api.anthropic.com/v1'
      ANTHROPIC_API_KEY: '${ANTHROPIC_API_KEY:-}'

      DEEPSEEK_API_URL: 'https://api.deepseek.com/v1'
      DEEPSEEK_API_KEY: '${DEEPSEEK_API_KEY:-}'

      COHERE_API_URL: 'https://api.cohere.ai'
      COHERE_API_KEY: '${COHERE_API_KEY:-}'

      # Optional: Ollama for local models
      OLLAMA_API_URL: 'http://ollama:11434/v1'
      OLLAMA_API_KEY: ''
    depends_on:
      mcp-filesystem:
        condition: service_healthy
      # mcp-web-search:
      #   condition: service_healthy
      mcp-context7:
        condition: service_healthy
      mcp-npm:
        condition: service_healthy
      mcp-memory:
        condition: service_healthy
      mcp-brave-search:
        condition: service_healthy
    networks:
      - inference-network
    pull_policy: always
    restart: unless-stopped

  mcp-filesystem:
    build:
      context: ./mcp-servers/filesystem
      dockerfile_inline: |
        FROM node:22-alpine
        WORKDIR /app
        RUN apk add --no-cache curl
        COPY package.json ./
        RUN npm install
        COPY . .
        EXPOSE 3000
        CMD ["npm", "start"]
    environment:
      MCP_SERVER_NAME: 'filesystem'
      MCP_SERVER_VERSION: '1.0.0'
      ALLOWED_DIRECTORIES: '/tmp'
      NODE_ENV: 'production'
    networks:
      - inference-network
    volumes:
      - ./shared:/tmp
    healthcheck:
      test: ['CMD', 'curl', '-f', 'http://localhost:3000/health']
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 45s
    restart: unless-stopped

  # mcp-web-search:
  #   build:
  #     context: ./mcp-servers/web-search
  #     dockerfile_inline: |
  #       FROM node:22-alpine
  #       WORKDIR /app
  #       RUN apk add --no-cache curl
  #       COPY package.json ./
  #       RUN npm install
  #       COPY . .
  #       EXPOSE 3001
  #       CMD ["npm", "start"]
  #   environment:
  #     NODE_ENV: 'production'
  #     MCP_SERVER_NAME: 'web-search'
  #     MCP_SERVER_VERSION: '1.0.0'
  #     PORT: '3001'
  #     HOST: '0.0.0.0'
  #   networks:
  #     - inference-network
  #   healthcheck:
  #     test: ['CMD', 'curl', '-f', 'http://localhost:3001/health']
  #     interval: 30s
  #     timeout: 10s
  #     retries: 5
  #     start_period: 45s
  #   restart: unless-stopped

  mcp-context7:
    build:
      context: ./mcp-servers/context7
      dockerfile_inline: |
        FROM node:22-alpine
        WORKDIR /app
        RUN apk add --no-cache curl
        COPY package.json ./
        RUN npm install
        COPY . .
        EXPOSE 3002
        CMD ["npm", "start"]
    environment:
      NODE_ENV: 'production'
      MCP_SERVER_NAME: 'context7'
      MCP_SERVER_VERSION: '1.0.0'
      PORT: '3002'
    networks:
      - inference-network
    healthcheck:
      test: ['CMD', 'curl', '-f', 'http://localhost:3002/health']
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 45s
    restart: unless-stopped

  mcp-npm:
    build:
      context: ./mcp-servers/npm
      dockerfile_inline: |
        FROM node:18-slim
        WORKDIR /app
        RUN apt-get update && apt-get install -y \
          curl \
          git \
          && rm -rf /var/lib/apt/lists/*
        COPY package.json ./
        RUN npm install
        COPY . .
        EXPOSE 3003
        CMD ["npm", "start"]
    environment:
      NODE_ENV: 'production'
      MCP_SERVER_NAME: 'npm'
      MCP_SERVER_VERSION: '1.0.0'
      PORT: '3003'
      ALLOWED_DIRECTORIES: '/tmp'
    networks:
      - inference-network
    volumes:
      - ./shared:/tmp
    healthcheck:
      test: ['CMD', 'curl', '-f', 'http://localhost:3003/health']
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 45s
    restart: unless-stopped

  mcp-memory:
    build:
      context: ./mcp-servers/memory
      dockerfile_inline: |
        FROM node:22-alpine
        WORKDIR /app
        RUN apk add --no-cache curl
        COPY package.json ./
        RUN npm install
        COPY . .
        EXPOSE 3004
        CMD ["npm", "start"]
    environment:
      NODE_ENV: 'production'
      MCP_SERVER_NAME: 'memory'
      MCP_SERVER_VERSION: '1.0.0'
      PORT: '3004'
      MEMORY_DIR: '/tmp/memory'
    networks:
      - inference-network
    volumes:
      - ./shared:/tmp
    healthcheck:
      test: ['CMD', 'curl', '-f', 'http://localhost:3004/health']
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 45s
    restart: unless-stopped

  # mcp-inspector:
  #   build:
  #     context: .
  #     dockerfile_inline: |
  #       FROM node:22-alpine
  #       WORKDIR /app
  #       RUN apk add --no-cache curl git
  #       RUN npm install -g @modelcontextprotocol/inspector
  #       EXPOSE 6274
  #       CMD ["npx", "@modelcontextprotocol/inspector"]
  #   environment:
  #     NODE_ENV: 'production'
  #     CLIENT_PORT: '6274'
  #   ports:
  #     - '6274:6274'
  #   networks:
  #     - inference-network
  #   depends_on:
  #     mcp-filesystem:
  #       condition: service_healthy
  #     mcp-web-search:
  #       condition: service_healthy
  #     mcp-context7:
  #       condition: service_healthy
  #     mcp-npm:
  #       condition: service_healthy
  #     mcp-memory:
  #       condition: service_healthy
  #     mcp-brave-search:
  #       condition: service_healthy
  #   healthcheck:
  #     test: ['CMD', 'curl', '-f', 'http://localhost:6274']
  #     interval: 30s
  #     timeout: 10s
  #     retries: 5
  #     start_period: 45s
  #   restart: unless-stopped

  mcp-brave-search:
    build:
      context: ./mcp-servers/brave-search
      dockerfile_inline: |
        FROM node:22-alpine
        WORKDIR /app
        RUN apk add --no-cache curl
        COPY package.json ./
        RUN npm install
        COPY . .
        EXPOSE 3005
        CMD ["npm", "start"]
    environment:
      NODE_ENV: 'production'
      MCP_SERVER_NAME: 'brave-search'
      MCP_SERVER_VERSION: '1.0.0'
      MCP_TRANSPORT: 'streamableHttp'
      PORT: '3005'
      BRAVE_API_KEY: '${BRAVE_API_KEY:-}'
    ports:
      - '3005:3005'
    networks:
      - inference-network
    healthcheck:
      test: ['CMD', 'curl', '-f', 'http://localhost:3005/health']
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 45s
    restart: unless-stopped

  # # Optional: Ollama for local models
  # ollama:
  #   image: ollama/ollama:latest
  #   ports:
  #     - '11434:11434'
  #   networks:
  #     - inference-network
  #   volumes:
  #     - ollama-data:/root/.ollama
  #   environment:
  #     OLLAMA_HOST: '0.0.0.0'
  #   restart: unless-stopped

volumes:
  shared-data:
    driver: local
  ollama-data:
    driver: local

networks:
  inference-network:
    driver: bridge
